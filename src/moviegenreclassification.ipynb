{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9640645,"sourceType":"datasetVersion","datasetId":5887000}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unidecode transformers tensorflow-addons\n!pip uninstall -y torch torchvision torchaudio transformers\n!pip install torch\n!pip install transformers==4.30.2\n!pip install unidecode datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:53:10.586538Z","iopub.execute_input":"2024-10-16T14:53:10.587199Z","iopub.status.idle":"2024-10-16T14:54:55.200338Z","shell.execute_reply.started":"2024-10-16T14:53:10.587158Z","shell.execute_reply":"2024-10-16T14:54:55.199128Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (1.3.8)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.10/site-packages (0.23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typeguard<3.0.0,>=2.7 in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (2.13.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nFound existing installation: torch 2.4.1\nUninstalling torch-2.4.1:\n  Successfully uninstalled torch-2.4.1\n\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: transformers 4.30.2\nUninstalling transformers-4.30.2:\n  Successfully uninstalled transformers-4.30.2\nCollecting torch\n  Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.10/site-packages (from torch) (3.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nUsing cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\neasyocr 1.7.2 requires torchvision>=0.5, which is not installed.\nfastai 2.7.17 requires torchvision>=0.11, which is not installed.\ntimm 1.0.9 requires torchvision, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.4.1\nCollecting transformers==4.30.2\n  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2.32.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.2) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (2024.8.30)\nUsing cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\nInstalling collected packages: transformers\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed transformers-4.30.2\nRequirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (1.3.8)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Install necessary packages\n\n# Import libraries\nimport torch\nimport pandas as pd\nimport numpy as np\nimport re\nfrom unidecode import unidecode\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import DistilBertTokenizer, DistilBertModel, Trainer, TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback\nfrom datasets import Dataset\ndevice = torch.device(\"cuda\") \n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA version:\", torch.version.cuda)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-16T14:54:55.202987Z","iopub.execute_input":"2024-10-16T14:54:55.203773Z","iopub.status.idle":"2024-10-16T14:54:55.211727Z","shell.execute_reply.started":"2024-10-16T14:54:55.203726Z","shell.execute_reply":"2024-10-16T14:54:55.210801Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"PyTorch version: 2.4.1+cu121\nCUDA available: True\nCUDA version: 12.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**2. Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"# Download NLTK stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\n# Define text cleaning functions\ndef remove_non_alphanum(string):\n    if isinstance(string, str):\n        string = unidecode(string)\n        string = re.sub(r'[^a-zA-Z0-9\\s]', '', string)\n    return string\n\ndef lowercase_and_remove_stopwords(string):\n    words = string.lower().split()\n    filtered_words = [word for word in words if word not in stop_words]\n    return ' '.join(filtered_words)\n\n# Load the data\ntrain = pd.read_csv('/kaggle/input/movie-plots/train.txt', \n                    delimiter='\\t', \n                    names=[\"Title\", \"Industry\", \"Genre\", \"Director\", \"Plot\"])\ntest = pd.read_csv('/kaggle/input/movie-plots/test_no_labels.txt', \n                   delimiter='\\t', \n                   names=[\"Title\", \"Industry\", \"Director\", \"Plot\"])\n\n# Clean the 'Plot' and 'Director' columns\nfor df in [train, test]:\n    df['Plot'] = df['Plot'].apply(remove_non_alphanum).apply(lowercase_and_remove_stopwords)\n    df['Director'] = df['Director'].apply(remove_non_alphanum).apply(lowercase_and_remove_stopwords)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:54:55.212711Z","iopub.execute_input":"2024-10-16T14:54:55.212999Z","iopub.status.idle":"2024-10-16T14:55:00.264327Z","shell.execute_reply.started":"2024-10-16T14:54:55.212970Z","shell.execute_reply":"2024-10-16T14:55:00.263442Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**3. Handle Class Imbalance **","metadata":{}},{"cell_type":"code","source":"# Check for class imbalance\ngenre_counts = train['Genre'].value_counts()\nprint(\"Genre counts in training data:\")\nprint(genre_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:55:00.266576Z","iopub.execute_input":"2024-10-16T14:55:00.266905Z","iopub.status.idle":"2024-10-16T14:55:00.275828Z","shell.execute_reply.started":"2024-10-16T14:55:00.266869Z","shell.execute_reply":"2024-10-16T14:55:00.274841Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Genre counts in training data:\nGenre\ndrama        1676\ncomedy       1193\nhorror       1108\naction       1059\nromance       886\nwestern       829\ncrime         541\nanimation     535\nsci-fi        214\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**3. Data Preparation**","metadata":{}},{"cell_type":"markdown","source":"a. Split Data into Training and Validation Sets","metadata":{}},{"cell_type":"code","source":"def split_data(data):\n    # Use only the first 100 samples for faster execution\n    train_data, tmp_data = train_test_split(\n        data, \n        test_size=0.2, \n        shuffle=True,\n        random_state=42\n    )\n\n    validation_data, test_data = train_test_split(\n        tmp_data, \n        test_size=0.5, \n        shuffle=True,\n        random_state=42\n    )\n    return train_data, validation_data, test_data\n\ntrain_data, val_data, test_data = split_data(train)","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:55:00.277059Z","iopub.execute_input":"2024-10-16T14:55:00.277678Z","iopub.status.idle":"2024-10-16T14:55:00.292704Z","shell.execute_reply.started":"2024-10-16T14:55:00.277635Z","shell.execute_reply":"2024-10-16T14:55:00.291743Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"b. Encode Labels and Directors","metadata":{}},{"cell_type":"code","source":"# Encode genres\nlabel_encoder = LabelEncoder()\ntrain_data['label'] = label_encoder.fit_transform(train_data['Genre'])\nval_data['label'] = label_encoder.transform(val_data['Genre'])\ntest_data['label'] = label_encoder.transform(test_data['Genre'])","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:55:00.294288Z","iopub.execute_input":"2024-10-16T14:55:00.294580Z","iopub.status.idle":"2024-10-16T14:55:00.303950Z","shell.execute_reply.started":"2024-10-16T14:55:00.294549Z","shell.execute_reply":"2024-10-16T14:55:00.302949Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**4. Tokenization**","metadata":{}},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntokenizer.add_special_tokens({'additional_special_tokens': ['[DIRECTOR]']})\n\n# Function to tokenize data\ndef tokenize_function(examples):\n    director_names = [f\"[DIRECTOR] {director}\" for director in examples['Director']]\n    texts_with_directors = [f\"{director_name} {text}\" for director_name, text in zip(director_names, examples['Plot'])]\n    return tokenizer(texts_with_directors, padding='max_length', truncation=True, max_length=512)\n\n# Prepare the datasets for Hugging Face\ntrain_dataset = Dataset.from_pandas(train_data[['Plot', 'Director', 'label']])\nval_dataset = Dataset.from_pandas(val_data[['Plot', 'Director', 'label']])\ntest_dataset = Dataset.from_pandas(test_data[['Plot', 'Director', 'label']])\n\n# Apply the tokenization\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Set the format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:55:00.305496Z","iopub.execute_input":"2024-10-16T14:55:00.305860Z","iopub.status.idle":"2024-10-16T14:56:19.939293Z","shell.execute_reply.started":"2024-10-16T14:55:00.305817Z","shell.execute_reply":"2024-10-16T14:56:19.938347Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6432 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b76cdd7884c483d971c5930a63c7bb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/804 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f148e17f5504b9b8f7b55d8611a01e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/805 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2219858e7b4ca4a351929d4665692b"}},"metadata":{}}]},{"cell_type":"markdown","source":"**5. Define the model**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n# Define the model class\nclass DistilBertForGenreClassification(nn.Module):\n    def __init__(self, num_labels):\n        super(DistilBertForGenreClassification, self).__init__()\n        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(self.bert.config.hidden_size, num_labels)\n        )\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # Get BERT output\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = bert_outputs.last_hidden_state[:, 0, :]  # CLS token output\n        # Get logits from classifier\n        logits = self.classifier(pooled_output)\n        # Compute loss if labels are provided\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n        return {'loss': loss, 'logits': logits}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:59.632876Z","iopub.execute_input":"2024-10-16T15:25:59.633284Z","iopub.status.idle":"2024-10-16T15:25:59.641594Z","shell.execute_reply.started":"2024-10-16T15:25:59.633244Z","shell.execute_reply":"2024-10-16T15:25:59.640688Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**6. Instantiate and Train the Model**","metadata":{}},{"cell_type":"code","source":"# Instantiate the model\nnum_labels = len(label_encoder.classes_)\nmodel = DistilBertForGenreClassification(num_labels=num_labels)\nmodel.bert.resize_token_embeddings(len(tokenizer))\n# model = model.to(device)\n\n# Check if the model's parameters are on the GPU\n#for param in model.parameters():\n    #print(param.device)\n    \n    \n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=2e-5,\n    num_train_epochs=10,  # Reduced for faster execution\n    # per_device_train_batch_size=4,  # Adjust batch size as needed\n    # per_device_eval_batch_size=4,\n    weight_decay=0.1,\n    load_best_model_at_end=True,\n    metric_for_best_model='accuracy',\n    greater_is_better=True,\n    logging_dir='./logs',\n    logging_steps=10,\n    report_to=[],\n)\n\n# Define compute_metrics function for evaluation\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    accuracy = (predictions == labels).mean()\n    return {'accuracy': accuracy}\n\n# Data collator\ndata_collator = DataCollatorWithPadding(tokenizer)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n# Train the model\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:26:09.714064Z","iopub.execute_input":"2024-10-16T15:26:09.714449Z","iopub.status.idle":"2024-10-16T15:41:45.955422Z","shell.execute_reply.started":"2024-10-16T15:26:09.714411Z","shell.execute_reply":"2024-10-16T15:41:45.954541Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4020' max='8040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4020/8040 15:33 < 15:34, 4.30 it/s, Epoch 5/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.212700</td>\n      <td>0.906176</td>\n      <td>0.670398</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.642600</td>\n      <td>0.894131</td>\n      <td>0.690299</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.520400</td>\n      <td>0.915814</td>\n      <td>0.715174</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.258400</td>\n      <td>1.261523</td>\n      <td>0.696517</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.352600</td>\n      <td>1.498350</td>\n      <td>0.707711</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2254: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4020, training_loss=0.5522524178250512, metrics={'train_runtime': 933.9738, 'train_samples_per_second': 68.867, 'train_steps_per_second': 8.608, 'total_flos': 0.0, 'train_loss': 0.5522524178250512, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"**7. Evaluate the Model on Test Data**","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"# Evaluate the model's performance on the test data\nmetrics = trainer.evaluate(test_dataset)\nprint(metrics)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:42:37.787272Z","iopub.execute_input":"2024-10-16T15:42:37.787694Z","iopub.status.idle":"2024-10-16T15:42:45.066186Z","shell.execute_reply.started":"2024-10-16T15:42:37.787654Z","shell.execute_reply":"2024-10-16T15:42:45.065275Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='101' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [101/101 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.8825925588607788, 'eval_accuracy': 0.7329192546583851, 'eval_runtime': 7.2712, 'eval_samples_per_second': 110.711, 'eval_steps_per_second': 13.89, 'epoch': 5.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**8. Make Predictions on the Test Set**","metadata":{}},{"cell_type":"code","source":"def test_sample(test_dataset, model):\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)  # Move model to device\n    \n    # Use DataLoader for batching\n    from torch.utils.data import DataLoader\n    test_dataloader = DataLoader(test_dataset, batch_size=4)\n    \n    predictions = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)        # Move input_ids to device\n            attention_mask = batch['attention_mask'].to(device)  # Move attention_mask to device\n            \n            # Forward pass\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs['logits']\n            preds = torch.softmax(logits, dim=1)\n            \n            # Move predictions to CPU and convert to numpy\n            predictions.extend(preds.cpu().numpy())\n    return np.array(predictions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:24:38.949211Z","iopub.execute_input":"2024-10-16T15:24:38.949955Z","iopub.status.idle":"2024-10-16T15:24:38.957361Z","shell.execute_reply.started":"2024-10-16T15:24:38.949915Z","shell.execute_reply":"2024-10-16T15:24:38.956403Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"9. Interpret Predictions and Calculate Metrics","metadata":{}},{"cell_type":"code","source":"# Decode predictions to labels\npredicted_class_indices = np.argmax(test_predictions, axis=1)\npredicted_classes = label_encoder.inverse_transform(predicted_class_indices)\n\n# True labels\ntrue_class_indices = test_dataset['label']\ntrue_classes = label_encoder.inverse_transform(true_class_indices)\n\n# Calculate accuracy and F1 score\nfrom sklearn.metrics import accuracy_score, f1_score\n\naccuracy = accuracy_score(true_classes, predicted_classes)\nf1 = f1_score(true_classes, predicted_classes, average='weighted')\n\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\nprint(f\"Test F1 Score: {f1:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T15:25:20.186117Z","iopub.execute_input":"2024-10-16T15:25:20.187199Z","iopub.status.idle":"2024-10-16T15:25:20.233320Z","shell.execute_reply.started":"2024-10-16T15:25:20.187141Z","shell.execute_reply":"2024-10-16T15:25:20.232157Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Decode predictions to labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_class_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mtest_predictions\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m predicted_classes \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(predicted_class_indices)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# True labels\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'test_predictions' is not defined"],"ename":"NameError","evalue":"name 'test_predictions' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"**10. Make Predictions on the Unlabeled Test Data**","metadata":{}},{"cell_type":"code","source":"# Prepare the unlabeled test data\nunlabeled_test = test.copy()  # Assuming 'test' is your test_no_labels.txt data\n\n# Since we don't have labels, we can use the same process without labels\nunlabeled_test_dataset = Dataset.from_pandas(unlabeled_test[['Plot', 'Director']])\n\n# Tokenize the data\nunlabeled_test_dataset = unlabeled_test_dataset.map(tokenize_function, batched=True)\n\n# Set the format for PyTorch\nunlabeled_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# Get predictions\ndef test_sample_unlabeled(test_dataset, model):\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)  # Move model to device\n    \n    # Use DataLoader for batching\n    from torch.utils.data import DataLoader\n    test_dataloader = DataLoader(test_dataset, batch_size=4)\n    \n    predictions = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            input_ids = batch['input_ids'].to(device)        # Move input_ids to device\n            attention_mask = batch['attention_mask'].to(device)  # Move attention_mask to device\n            \n            # Forward pass\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs['logits']\n            preds = torch.softmax(logits, dim=1)\n            \n            # Move predictions to CPU and convert to numpy\n            predictions.extend(preds.cpu().numpy())\n    return np.array(predictions)\n\n\nunlabeled_predictions = test_sample_unlabeled(unlabeled_test_dataset, model)\n\n# Decode predictions to labels\nunlabeled_predicted_class_indices = np.argmax(unlabeled_predictions, axis=1)\nunlabeled_predicted_classes = label_encoder.inverse_transform(unlabeled_predicted_class_indices)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:57:10.079157Z","iopub.status.idle":"2024-10-16T14:57:10.079535Z","shell.execute_reply.started":"2024-10-16T14:57:10.079325Z","shell.execute_reply":"2024-10-16T14:57:10.079342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"11. Save the Results","metadata":{}},{"cell_type":"code","source":"# Write predictions to a text file\nwith open('results.txt', 'w') as f:\n    for title, pred_label in zip(unlabeled_test['Title'], unlabeled_predicted_classes):\n        f.write(f\"{title}: {pred_label}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-16T14:57:10.081447Z","iopub.status.idle":"2024-10-16T14:57:10.081827Z","shell.execute_reply.started":"2024-10-16T14:57:10.081642Z","shell.execute_reply":"2024-10-16T14:57:10.081660Z"},"trusted":true},"execution_count":null,"outputs":[]}]}